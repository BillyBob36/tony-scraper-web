#!/usr/bin/env python3
"""
Script de scraping d√©taill√© des profils Tony
============================================

Ce script compl√®te les informations des profils en scrapant les pages individuelles.
Il extrait des donn√©es d√©taill√©es depuis les pages de profil comme :
- √âl√©ments cl√©s (effectifs, chiffre d'affaires, etc.)
- Secteur(s) d'activit√©
- Mission en une phrase
- Nombre de points de vente
- Solutions/comp√©tences recherch√©es

Utilisation :
    python scraping_tony_profil_detail.py [nombre_profils]
    
Arguments :
    nombre_profils : Nombre de profils √† traiter (0 pour tous, d√©faut: 2)

Exemples :
    python scraping_tony_profil_detail.py 5     # Traiter 5 profils
    python scraping_tony_profil_detail.py 0     # Tous les profils
    python scraping_tony_profil_detail.py       # 2 profils (d√©faut)
"""

import json
import csv
import time
import sys
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TonyProfilDetailScraper:
    def __init__(self, nombre_profils=2):
        self.nombre_profils = nombre_profils
        self.driver = None
        self.wait = None
        self.profils_complets = []
        
    def setup_driver(self):
        """Configure le driver Selenium"""
        options = webdriver.ChromeOptions()
        options.add_argument('--window-size=1920,1080')
        # options.add_argument('--headless')  # Comment√© pour voir le navigateur
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def load_profils_from_json(self):
        """Charge les profils depuis le fichier JSON"""
        try:
            with open('profils_tony.json', 'r', encoding='utf-8') as f:
                profils = json.load(f)
            logger.info(f"‚úÖ {len(profils)} profils charg√©s depuis profils_tony.json")
            return profils
        except FileNotFoundError:
            logger.error("‚ùå Fichier profils_tony.json introuvable")
            return []
        except json.JSONDecodeError:
            logger.error("‚ùå Erreur de lecture du JSON")
            return []
    
    def extraire_elements_cles(self):
        """Extrait les √©l√©ments cl√©s depuis la section d√©di√©e"""
        elements_cles = {}
        
        try:
            # Trouver la section √âl√©ments cl√©s
            section = self.driver.find_element(By.CSS_SELECTOR, "#object-d6fa1ac7 .section__content")
            items = section.find_elements(By.TAG_NAME, "li")
            
            for item in items:
                text = item.text.strip()
                if text.startswith("Effectifs"):
                    elements_cles['effectifs'] = text.replace("Effectifs : ", "")
                elif text.startswith("Chiffre d'affaires"):
                    elements_cles['chiffre_affaires'] = text.replace("Chiffre d'affaires : ", "")
                elif "Transformation digitale" in text or "Retail tech" in text:
                    elements_cles['competences'] = text
                    
        except NoSuchElementException:
            logger.warning("‚ö†Ô∏è Section √âl√©ments cl√©s non trouv√©e")
            
        return elements_cles
    
    def extraire_secteur_activite(self):
        """Extrait le secteur d'activit√©"""
        try:
            secteur_element = self.driver.find_element(
                By.CSS_SELECTOR, 
                "#object-Me3f9M9edd .section__content li.highlight"
            )
            return secteur_element.text.strip()
        except NoSuchElementException:
            logger.warning("‚ö†Ô∏è Secteur d'activit√© non trouv√©")
            return None
    
    def extraire_mission(self):
        """Extrait la mission en une phrase"""
        try:
            mission_element = self.driver.find_element(
                By.CSS_SELECTOR,
                "#object-M4561Macb7 .section__content li"
            )
            return mission_element.text.strip()
        except NoSuchElementException:
            logger.warning("‚ö†Ô∏è Mission non trouv√©e")
            return None
    
    def extraire_nombre_points_vente(self):
        """Extrait le nombre de points de vente"""
        try:
            points_vente_element = self.driver.find_element(
                By.CSS_SELECTOR,
                "#object-M91ceM1169 .section__content li.highlight"
            )
            return points_vente_element.text.strip()
        except NoSuchElementException:
            logger.warning("‚ö†Ô∏è Nombre de points de vente non trouv√©")
            return None
    
    def extraire_solutions_competences(self):
        """Extrait les solutions/comp√©tences recherch√©es"""
        try:
            solutions_element = self.driver.find_element(
                By.CSS_SELECTOR,
                "#object-M184bM50c8 .section__content li.highlight"
            )
            return solutions_element.text.strip()
        except NoSuchElementException:
            logger.warning("‚ö†Ô∏è Solutions/comp√©tences non trouv√©es")
            return None
    
    def scraper_profil_detail(self, profil_url):
        """Scrape une page de profil individuelle"""
        try:
            logger.info(f"üåê Acc√®s √† : {profil_url}")
            self.driver.get(profil_url)
            
            # Attendre le chargement de la page
            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(2)  # Temps suppl√©mentaire pour le chargement dynamique
            
            # Extraire toutes les informations
            details = {
                'url_profil': profil_url,
                'scraped_at': datetime.now().isoformat(),
                'elements_cles': self.extraire_elements_cles(),
                'secteur_activite': self.extraire_secteur_activite(),
                'mission': self.extraire_mission(),
                'nombre_points_vente': self.extraire_nombre_points_vente(),
                'solutions_competences': self.extraire_solutions_competences()
            }
            
            logger.info("‚úÖ Informations d√©taill√©es extraites")
            return details
            
        except TimeoutException:
            logger.error(f"‚ùå Timeout lors du chargement de {profil_url}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du scraping : {str(e)}")
            return None
    
    def enrichir_profils(self):
        """Enrichit les profils avec les d√©tails"""
        profils_base = self.load_profils_from_json()
        
        if not profils_base:
            return
            
        # D√©terminer le nombre de profils √† traiter
        if self.nombre_profils == 0:
            profils_a_traiter = profils_base
        else:
            profils_a_traiter = profils_base[:self.nombre_profils]
            
        logger.info(f"üéØ Traitement de {len(profils_a_traiter)} profils")
        
        for index, profil in enumerate(profils_a_traiter, 1):
            nom_prenom = profil.get('nom_prenom', 'Inconnu')
            entreprise = profil.get('entreprise', '')
            logger.info(f"\nüìã Profil {index}/{len(profils_a_traiter)}: {nom_prenom} - {entreprise}")
            
            # R√©cup√©rer l'URL du profil
            profil_url = profil.get('url_profil')
            
            if not profil_url:
                logger.warning(f"‚ö†Ô∏è Aucune URL de profil trouv√©e pour {nom_prenom}")
                continue
                
            # Scraper les d√©tails
            details = self.scraper_profil_detail(profil_url)
            
            if details:
                # Combiner les informations
                profil_complet = {**profil, **details}
                self.profils_complets.append(profil_complet)
                logger.info("‚úÖ Profil enrichi avec succ√®s")
            
            # Pause entre les profils pour √©viter le blocage
            time.sleep(1)
    
    def sauvegarder_resultats(self):
        """Sauvegarde les r√©sultats en JSON et CSV"""
        if not self.profils_complets:
            logger.warning("‚ö†Ô∏è Aucun profil √† sauvegarder")
            return
            
        timestamp = int(time.time())
        
        # Sauvegarder en JSON
        json_filename = f'profils_tony_complets_{timestamp}.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.profils_complets, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ Sauvegard√© : {json_filename}")
        
        # Sauvegarder en CSV
        csv_filename = f'profils_tony_complets_{timestamp}.csv'
        if self.profils_complets:
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.profils_complets[0].keys())
                writer.writeheader()
                writer.writerows(self.profils_complets)
            logger.info(f"‚úÖ Sauvegard√© : {csv_filename}")
    
    def run(self):
        """Ex√©cute le scraping complet"""
        try:
            logger.info("üöÄ D√©marrage du scraping d√©taill√© des profils")
            self.setup_driver()
            
            if self.nombre_profils == 0:
                logger.info("üîÑ Mode : tous les profils")
            else:
                logger.info(f"üéØ Mode : {self.nombre_profils} premier(s) profil(s)")
            
            self.enrichir_profils()
            self.sauvegarder_resultats()
            
            logger.info(f"‚úÖ Scraping termin√© : {len(self.profils_complets)} profils enrichis")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ex√©cution : {str(e)}")
        finally:
            if self.driver:
                self.driver.quit()

def main():
    """Fonction principale avec gestion des arguments"""
    nombre_profils = 2  # Valeur par d√©faut
    
    if len(sys.argv) > 1:
        try:
            nombre_profils = int(sys.argv[1])
            if nombre_profils < 0:
                print("‚ùå Le nombre de profils doit √™tre positif ou 0")
                sys.exit(1)
        except ValueError:
            print("‚ùå L'argument doit √™tre un nombre entier")
            sys.exit(1)
    
    scraper = TonyProfilDetailScraper(nombre_profils)
    scraper.run()

if __name__ == "__main__":
    main()